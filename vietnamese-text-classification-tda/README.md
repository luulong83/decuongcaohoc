# Project Structure
ğŸ¯ GIáº¢I PHÃP Tá»”NG THá»‚
Kiáº¿n trÃºc 3-táº§ng

vietnamese-text-classification-tda/
â”œâ”€â”€ ğŸ“Š Data Layer (Preprocessing + Augmentation)
â”œâ”€â”€ ğŸ§  Model Layer (PhoBERT + TDA + Fusion)
â””â”€â”€ ğŸ”¬ Experiment Layer (Training + Evaluation + Logging)


`CÃ¡c Components chÃ­nh`

1. TDA Module (Persistent Homology + Persistence Images)
2. Data Augmentation Module (SR + BT + Contextual)
3. PhoBERT Integration (Attention extraction + Feature fusion)
4. Training Pipeline (Checkpoint + Resume + Logging)
5. Evaluation Suite (Metrics + Visualization + Statistical tests)

```
vietnamese-text-classification-tda/
â”‚
â”œâ”€â”€ README.md                          # HÆ°á»›ng dáº«n cÃ i Ä‘áº·t vÃ  sá»­ dá»¥ng
â”œâ”€â”€ requirements.txt                   # Dependencies
â”œâ”€â”€ setup.py                          # Package setup
â”œâ”€â”€ .gitignore                        
â”‚
â”œâ”€â”€ configs/                          # âš™ï¸ Configuration files
â”‚   â”œâ”€â”€ base_config.yaml              # Base configuration
â”‚   â”œâ”€â”€ experiment_configs/           
â”‚   â”‚   â”œâ”€â”€ e0_tfidf_svm.yaml        # E0: TF-IDF baseline
â”‚   â”‚   â”œâ”€â”€ e1_phobert_baseline.yaml # E1: PhoBERT baseline
â”‚   â”‚   â”œâ”€â”€ e2_phobert_da.yaml       # E2: PhoBERT + DA
â”‚   â”‚   â”œâ”€â”€ e3_phobert_tda.yaml      # E3: PhoBERT + TDA
â”‚   â”‚   â””â”€â”€ e4_proposed.yaml         # E4: Full model
â”‚   â””â”€â”€ ablation_configs/
â”‚       â”œâ”€â”€ a1_layer_selection.yaml
â”‚       â”œâ”€â”€ a2_homology_dims.yaml
â”‚       â”œâ”€â”€ a3_vectorization.yaml
â”‚       â”œâ”€â”€ a4_fusion_method.yaml
â”‚       â”œâ”€â”€ a5_da_technique.yaml
â”‚       â””â”€â”€ a6_da_ratio.yaml
â”‚
â”œâ”€â”€ data/                             # ğŸ“Š Data directory
â”‚   â”œâ”€â”€ raw/                          # Original UIT-VSFC
â”‚   â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ dev/
â”‚   â”‚   â””â”€â”€ test/
â”‚   â”œâ”€â”€ processed/                    # Preprocessed data
â”‚   â”œâ”€â”€ augmented/                    # Augmented data
â”‚   â””â”€â”€ cache/                        # Cached features
â”‚
â”œâ”€â”€ src/                              # ğŸ”§ Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                         # Data processing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dataset.py               # Custom Dataset class
â”‚   â”‚   â”œâ”€â”€ preprocessor.py          # Text preprocessing
â”‚   â”‚   â””â”€â”€ augmentation/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ synonym_replacement.py
â”‚   â”‚       â”œâ”€â”€ back_translation.py
â”‚   â”‚       â””â”€â”€ contextual_aug.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                       # Model definitions
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ phobert_classifier.py    # PhoBERT baseline
â”‚   â”‚   â”œâ”€â”€ tda_module.py            # TDA feature extraction
â”‚   â”‚   â”œâ”€â”€ fusion_model.py          # Feature fusion
â”‚   â”‚   â””â”€â”€ baselines/
â”‚   â”‚       â”œâ”€â”€ tfidf_svm.py         # E0: Traditional baseline
â”‚   â”‚       â”œâ”€â”€ lstm.py
â”‚   â”‚       â””â”€â”€ cnn.py
â”‚   â”‚
â”‚   â”œâ”€â”€ tda/                          # TDA components
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ persistent_homology.py   # PH computation
â”‚   â”‚   â”œâ”€â”€ persistence_images.py    # PI vectorization
â”‚   â”‚   â”œâ”€â”€ attention_processor.py   # Attention map processing
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”‚
â”‚   â”œâ”€â”€ training/                     # Training pipeline
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ trainer.py               # Main trainer
â”‚   â”‚   â”œâ”€â”€ checkpoint.py            # Checkpoint management
â”‚   â”‚   â””â”€â”€ early_stopping.py
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/                   # Evaluation suite
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ metrics.py               # Accuracy, F1, etc.
â”‚   â”‚   â”œâ”€â”€ statistical_tests.py     # t-test, Cohen's d
â”‚   â”‚   â””â”€â”€ visualization.py         # Plots and figures
â”‚   â”‚
â”‚   â””â”€â”€ utils/                        # Utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py                # Custom logger
â”‚       â”œâ”€â”€ config_loader.py         # Config management
â”‚       â””â”€â”€ gpu_utils.py             # GPU/CPU detection
â”‚
â”œâ”€â”€ scripts/                          # ğŸš€ Execution scripts
â”‚   â”œâ”€â”€ run_experiment.py            # Main experiment runner
â”‚   â”œâ”€â”€ run_ablation.py              # Ablation study runner
â”‚   â”œâ”€â”€ preprocess_data.py           # Data preprocessing
â”‚   â”œâ”€â”€ augment_data.py              # Data augmentation
â”‚   â””â”€â”€ evaluate_model.py            # Model evaluation
â”‚
â”œâ”€â”€ notebooks/                        # ğŸ““ Jupyter notebooks
â”‚   â”œâ”€â”€ 01_eda.ipynb                 # Exploratory Data Analysis
â”‚   â”œâ”€â”€ 02_tda_exploration.ipynb     # TDA visualization
â”‚   â”œâ”€â”€ 03_model_analysis.ipynb      # Model interpretation
â”‚   â””â”€â”€ 04_results_visualization.ipynb
â”‚
â”œâ”€â”€ tests/                            # ğŸ§ª Unit tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_data_processing.py
â”‚   â”œâ”€â”€ test_tda_module.py
â”‚   â”œâ”€â”€ test_augmentation.py
â”‚   â””â”€â”€ test_models.py
â”‚
â”œâ”€â”€ experiments/                      # ğŸ“ˆ Experiment outputs
â”‚   â”œâ”€â”€ logs/                        # Training logs
â”‚   â”œâ”€â”€ checkpoints/                 # Model checkpoints
â”‚   â”œâ”€â”€ results/                     # Evaluation results
â”‚   â””â”€â”€ visualizations/              # Figures and plots
â”‚
â””â”€â”€ docs/                            # ğŸ“š Documentation
    â”œâ”€â”€ INSTALL.md                   # Installation guide
    â”œâ”€â”€ USAGE.md                     # Usage guide
    â”œâ”€â”€ API.md                       # API documentation
    â””â”€â”€ EXPERIMENTS.md               # Experiment documentation
```

---

## Key Files Description

### Core Modules

#### `src/models/phobert_classifier.py`
PhoBERT baseline model with:
- Attention map extraction (layers 8-11)
- CLS token feature extraction
- Classification head

#### `src/tda/persistent_homology.py`
TDA computation:
- Vietoris-Rips filtration
- Persistent homology (Hâ‚€, Hâ‚)
- Stability computation

#### `src/tda/persistence_images.py`
Vectorization:
- 20Ã—20 grid Persistence Images
- Gaussian smoothing (Ïƒ=0.1)
- Normalization

#### `src/data/augmentation/`
Data augmentation:
- **synonym_replacement.py**: Tone-aware Vietnamese synonyms
- **back_translation.py**: Viâ†’Enâ†’Vi with quality check
- **contextual_aug.py**: PhoBERT MLM-based augmentation

#### `src/training/trainer.py`
Training pipeline:
- Checkpoint saving/loading (resume capability)
- Early stopping
- Learning rate scheduling
- Gradient accumulation

#### `src/utils/logger.py`
Comprehensive logging:
- Console output
- File logging (with rotation)
- TensorBoard integration
- Error tracking

### Experiment Scripts

#### `scripts/run_experiment.py`
```bash
# Run E0: TF-IDF baseline
python scripts/run_experiment.py --config configs/experiment_configs/e0_tfidf_svm.yaml

# Run E4: Proposed method
python scripts/run_experiment.py --config configs/experiment_configs/e4_proposed.yaml --gpu 0
```

#### `scripts/run_ablation.py`
```bash
# Run A1: Layer selection ablation
python scripts/run_ablation.py --config configs/ablation_configs/a1_layer_selection.yaml
```

---

## Platform Support

### Windows PC
```bash
# CPU-only mode
python scripts/run_experiment.py --config configs/experiment_configs/e1_phobert_baseline.yaml --device cpu

# With GPU (if available)
python scripts/run_experiment.py --config configs/experiment_configs/e4_proposed.yaml --device cuda
```

### Google Colab
```python
# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Clone repository
!git clone https://github.com/your-repo/vietnamese-text-classification-tda.git
%cd vietnamese-text-classification-tda

# Install dependencies
!pip install -r requirements.txt

# Run experiment with Colab GPU
!python scripts/run_experiment.py --config configs/experiment_configs/e4_proposed.yaml --device cuda
```

---

## Resume Training

```bash
# Resume from checkpoint
python scripts/run_experiment.py \
  --config configs/experiment_configs/e4_proposed.yaml \
  --resume experiments/checkpoints/e4_proposed_epoch10.pt
```

Checkpoint includes:
- Model state_dict
- Optimizer state_dict
- Epoch number
- Best validation score
- Training history

---

## Logging System

### Log Structure
```
experiments/logs/
â”œâ”€â”€ e4_proposed_20250212_143052.log  # Main log
â”œâ”€â”€ tensorboard/                     # TensorBoard logs
â”‚   â””â”€â”€ e4_proposed/
â””â”€â”€ errors/                          # Error logs
    â””â”€â”€ e4_proposed_errors.log
```

### Log Content
- Training progress (loss, accuracy per epoch)
- Validation metrics
- TDA computation time
- Data augmentation statistics
- GPU memory usage
- Error stacktraces

---

## Next Steps

1. **Install dependencies**: `pip install -r requirements.txt`
2. **Preprocess data**: `python scripts/preprocess_data.py`
3. **Run baseline**: `python scripts/run_experiment.py --config configs/experiment_configs/e1_phobert_baseline.yaml`
4. **Analyze results**: Open `notebooks/04_results_visualization.ipynb`